# An√°lise, Clusteriza√ß√£o e Categoriza√ß√£o de Artigos Cient√≠ficos do ArXiv

Este projeto apresenta uma metodologia para organiza√ß√£o e categoriza√ß√£o autom√°tica de grandes volumes de artigos cient√≠ficos atrav√©s de t√©cnicas avan√ßadas de processamento de linguagem natural e aprendizado de m√°quina. O objetivo principal √© transformar uma cole√ß√£o massiva de aproximadamente 222.259 artigos do arXiv em bibliotecas tem√°ticas especializadas, facilitando o acesso e a gest√£o do conhecimento cient√≠fico.

Aqui ser√° realizada uma an√°lise completa, clusteriza√ß√£o e categoriza√ß√£o de artigos cient√≠ficos do arXiv utilizando t√©cnicas de Processamento de Linguagem Natural (PLN) e aprendizado de m√°quina n√£o supervisionado.

A explos√£o de produ√ß√£o cient√≠fica representa um desafio significativo para organiza√ß√£o e recupera√ß√£o de informa√ß√£o. Este projeto aborda esse problema atrav√©s de uma abordagem sistem√°tica que combina embeddings de texto, redu√ß√£o de dimensionalidade e t√©cnicas de clusteriza√ß√£o para criar subconjuntos tem√°ticos coerentes a partir de uma grande biblioteca heterog√™nea.

## üìã Objetivo

Transformar uma cole√ß√£o massiva de aproximadamente **222.259 artigos cient√≠ficos do arXiv** em bibliotecas tem√°ticas especializadas, facilitando o acesso e a gest√£o do conhecimento cient√≠fico atrav√©s de:

- **An√°lise completa** dos dados textuais
- **Clusteriza√ß√£o** autom√°tica de artigos
- **Categoriza√ß√£o** tem√°tica inteligente
- **Visualiza√ß√£o** interativa dos resultados

## üõ†Ô∏è Tecnologias Utilizadas

### Principais Bibliotecas
- **LangChain** & **OpenAI Embeddings** - Para embeddings de texto sem√¢ntico
- **FAISS** - Armazenamento e busca eficiente em espa√ßos vetoriais
- **UMAP** - Redu√ß√£o de dimensionalidade
- **Scikit-learn** - Algoritmos de machine learning
- **TensorFlow/Keras** - Redes neurais profundas
- **NLTK** - Processamento de linguagem natural
- **pyLDAvis** - Visualiza√ß√£o de t√≥picos
- **Matplotlib/Seaborn** - Visualiza√ß√µes

### Modelos de IA
- **text-embedding-3-large** (OpenAI) - Gera√ß√£o de embeddings
- **K-Means** - Clusteriza√ß√£o
- **Random Forest** & **XGBoost** - Classifica√ß√£o
- **Redes Neurais** - Modelos profundos para categoriza√ß√£o

## üìä Metodologia

### 1. **Pr√©-processamento**
- Amostragem estratificada de 8.000 artigos (3,6% do total)
- Limpeza e normaliza√ß√£o de texto
- Remo√ß√£o de stopwords e tokeniza√ß√£o

### 2. **Embeddings de Texto**
- Convers√£o do conte√∫do textual em representa√ß√µes vetoriais densas
- Uso do modelo text-embedding-3-large da OpenAI
- Captura de sem√¢ntica e rela√ß√µes contextuais

### 3. **Redu√ß√£o de Dimensionalidade**
- **UMAP** (Uniform Manifold Approximation and Projection)
- Redu√ß√£o para 100 dimens√µes preservando estrutura local e global
- Par√¢metros otimizados: `n_neighbors=20`, `min_dist=0.001`, m√©trica cosseno

### 4. **Clusteriza√ß√£o**
- **K-Means** para agrupamento autom√°tico
- Valida√ß√£o com **Silhouette Score**
- Identifica√ß√£o de clusters tem√°ticos coerentes

### 5. **Visualiza√ß√£o**
- Proje√ß√µes 2D e 3D interativas
- Word clouds para an√°lise de t√≥picos
- Gr√°ficos de dispers√£o

## üóÇÔ∏è Estrutura dos Dados

Os artigos cont√™m:
- **T√≠tulo** e **autores**
- **DOI** e **data de atualiza√ß√£o**
- **Conte√∫do textual** completo
- **Embeddings** gerados (3072 dimens√µes)
- **Metadados** diversos

## üöÄ Como Executar

### Pr√©-requisitos
```bash
pip install langchain_community langchain_openai faiss-cpu tiktoken pyLDAvis
pip install umap-learn scikit-learn tensorflow xgboost
```

### Configura√ß√£o
1. Configure sua API key da OpenAI:
```python
os.environ["OPENAI_API_KEY"] = "sua_chave_aqui"
```

2. Carregue o banco de vetores FAISS:
```python
biblioteca = FAISS.load_local('caminho/para/embeddings', embeddings)
```

### Execu√ß√£o Principal
O notebook est√° organizado em se√ß√µes sequenciais:
1. Configura√ß√£o e importa√ß√µes
2. Carregamento dos dados
3. Pr√©-processamento e amostragem
4. Redu√ß√£o de dimensionalidade
5. Clusteriza√ß√£o e an√°lise
6. Visualiza√ß√£o dos resultados

## üìà Resultados

- **Clusters tem√°ticos** identificados automaticamente
- **Bibliotecas especializadas** organizadas por t√≥pico
- **Visualiza√ß√µes interativas** da distribui√ß√£o de artigos
- **Modelos de categoriza√ß√£o** treinados e validados
- **Insights** sobre a estrutura do conhecimento cient√≠fico

## üéØ Aplica√ß√µes

- **Organiza√ß√£o autom√°tica** de bibliotecas cient√≠ficas
- **Descoberta de t√≥picos** emergentes
- **Recomenda√ß√£o** de artigos relacionados
- **An√°lise de tend√™ncias** de pesquisa
- **Gest√£o do conhecimento** institucional

## üìä M√©tricas de Avalia√ß√£o

- **Silhouette Score** - Qualidade dos clusters
- **Coer√™ncia de t√≥picos** - An√°lise sem√¢ntica
- **Precis√£o/Recall** - Modelos de classifica√ß√£o
- **Visual qualitativo** - Interpretabilidade dos resultados

## üîÆ Poss√≠veis Melhorias

- Incorpora√ß√£o de **metadados adicionais**
- **Modelos transformers** mais avan√ßados
- **An√°lise temporal** de tend√™ncias
- **Sistema de recomenda√ß√£o** em tempo real
- **Interface web** interativa

# **Desempenho do Modelo de Classifica√ß√£o**

O modelo de classifica√ß√£o desenvolvido para categorizar os artigos cient√≠ficos nos clusters identificados alcan√ßou uma acur√°cia not√°vel de 97,1%. Este resultado excepcional demonstra a efic√°cia da abordagem proposta e a qualidade dos embeddings e clusters gerados na fase anterior do projeto.

**An√°lise do Resultado**

A alta acur√°cia de 97,1% indica que:

- Qualidade dos Embeddings: Os embeddings gerados pelo modelo text-embedding-3-large capturam efetivamente as caracter√≠sticas sem√¢nticas e tem√°ticas dos artigos, permitindo uma distin√ß√£o clara entre diferentes dom√≠nios cient√≠ficos.

- Efic√°cia da Clusteriza√ß√£o Inicial: A estrat√©gia de clusteriza√ß√£o na amostra de 8.000 artigos produziu grupos coerentes e bem delimitados, proporcionando r√≥tulos de alta qualidade para o treinamento supervisionado.

- Representatividade da Amostra: A amostra de 3,6% do total mostrou-se estatisticamente representativa, contendo padr√µes suficientes para generalizar para os 222.259 artigos.

- Adequa√ß√£o do Pipeline UMAP + Classifica√ß√£o: A redu√ß√£o dimensional com UMAP preservou informa√ß√µes relevantes para a distin√ß√£o entre categorias, facilitando o trabalho do classificador.

## **Implica√ß√µes Pr√°ticas**

Esta alta taxa de acur√°cia tem implica√ß√µes significativas para o sistema proposto:

- Confiabilidade na Organiza√ß√£o Autom√°tica: Com 97,1% de precis√£o, os usu√°rios podem confiar que os artigos est√£o sendo direcionados para as bibliotecas especializadas corretas.

- Redu√ß√£o de Erros de Categoriza√ß√£o: Apenas 2,9% dos artigos podem necessitar de revis√£o ou reclassifica√ß√£o, representando um custo computacional m√≠nimo para corre√ß√µes manuais.

- Escalabilidade Comprovada: O sucesso na classifica√ß√£o do restante dos artigos valida a estrat√©gia de usar uma amostra reduzida para treinamento.


# **Conclus√£o**

A abordagem proposta demonstra a viabilidade de organizar grandes cole√ß√µes cient√≠ficas atrav√©s de t√©cnicas modernas de PLN e aprendizado de m√°quina. A estrat√©gia de amostragem seguida de classifica√ß√£o em escala mostrou-se eficiente tanto computacionalmente quanto em termos de qualidade na categoriza√ß√£o, oferecendo uma solu√ß√£o escal√°vel para o desafio da gest√£o de grandes volumes de informa√ß√£o cient√≠fica.

---

*Desenvolvido para transformar grandes volumes de artigos cient√≠ficos em conhecimento estruturado e acess√≠vel.*
